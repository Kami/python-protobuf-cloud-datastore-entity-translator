[tox]
envlist = lint,py{2.7,3.6,3.7}-unit-tests,py3.7-integration-tests,coverage,{2.7,3.6,3.7}-benchmarks
skipsdist = true

[testenv]
basepython =
    {py3.7-unit-tests,py3.7-integration-tests,lint,mypy,coverage}: python3.7
    {py2.7-unit-tests,py2.7-benchmarks}: python2.7
    {py3.6-unit-tests,py3.6-benchmarks}: python3.6
    {py3.7-unit-tests,py3.7-benchmarks}: python3.7
install_command = pip install -U --force-reinstall {opts} {packages}
deps = -r requirements-test.txt
       -r requirements.txt
setenv =
    PYTHONPATH = {toxinidir}/tests/generated/
whitelist_externals =
    rm
commands =
    py.test --benchmark-disable -vv --durations=5 tests/unit/

[testenv:lint]
deps = -r requirements-test.txt
       -r requirements-examples.txt
       -r requirements.txt
       mypy
       mypy-protobuf
commands =
    flake8 --config ./lint-configs/.flake8 protobuf_cloud_datastore_translator/ tests/ examples/
    pylint -E --rcfile=./lint-configs/.pylintrc protobuf_cloud_datastore_translator/ tests/ examples/
    mypy --no-incremental --config-file lint-configs/mypy.ini protobuf_cloud_datastore_translator/ tests/unit/ tests/integration/

[testenv:mypy]
deps = -r requirements-test.txt
       -r requirements.txt
       mypy
       mypy-protobuf
commands =
    mypy --no-incremental --config-file lint-configs/mypy.ini protobuf_cloud_datastore_translator/ tests/unit/ tests/integration/ examples/

[testenv:py3.7-integration-tests]
commands =
    py.test --benchmark-disable -vv --durations=5 tests/integration/

[testenv:coverage]
commands =
    rm -f .coverage
    py.test --benchmark-disable --cov=protobuf_cloud_datastore_translator --cov=tests tests/unit/ tests/integration/ tests/test_benchmarks.py

[testenv:coverage-travis]
passenv = TOXENV CI TRAVIS TRAVIS_*
set-env =
commands =
    rm -f .coverage
    py.test --benchmark-disable --cov=protobuf_cloud_datastore_translator tests/unit/ tests/integration/ tests/test_benchmarks.py
    codecov

# Benchmark targets
# NOTE: We fail the tests if min run time is 5% slower for any test
# TODO Enable auto fail once we have initial run data
# --benchmark-compare --benchmark-compare-fail=min:5%
[testenv:py2.7-benchmarks]
commands =
    py.test --benchmark-autosave tests/test_benchmarks.py

[testenv:py3.6-benchmarks]
commands =
    py.test --benchmark-autosave tests/test_benchmarks.py

[testenv:py3.7-benchmarks]
commands =
    py.test --benchmark-autosave tests/test_benchmarks.py
